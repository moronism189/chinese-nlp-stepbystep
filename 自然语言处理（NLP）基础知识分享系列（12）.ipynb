{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad35e0e",
   "metadata": {},
   "source": [
    "## `åœ¨Huggingface transformerså¹³å°ä¸Šå¾®è°ƒBERT-wwm-ext`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d432bb",
   "metadata": {},
   "source": [
    "ä»Šå¤©æ˜¯æœ¬ç³»åˆ—çš„æœ€åä¸€æœŸã€‚\n",
    "\n",
    "### transformer ä¸ transformers \n",
    "\n",
    "ä¸ºäº†é¿å…å‘½åå¸¦æ¥çš„æ··æ·†ï¼Œæˆ‘ä»¬é¦–å…ˆæ¥å˜æ¸…ä¸€ä¸‹ï¼štransformer ä¸ transformersã€‚ \n",
    "\n",
    "#### â€¢ transformer \n",
    "\n",
    "åœ¨ä¸Šä¸€æœŸé‡Œï¼Œæˆ‘ä»¬å·²ç»åšè¿‡ä»‹ç»ï¼Œtransformeræ˜¯ä¸€ç§å…·æœ‰å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ã€å¯ä»¥å–ä»£RNN/LSTMçš„ç¥ç»ç½‘ç»œå•å…ƒç»“æ„ã€‚ \n",
    "\n",
    "***æœ¬è´¨ä¸Šå®ƒæ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æŠ€æœ¯ã€‚*** \n",
    "\n",
    "#### â€¢ transformers\n",
    "\n",
    "ä»Šå¤©æåˆ°çš„transformersï¼Œæ˜¯HuggingfaceğŸ¤—å…¬å¸å¼€å‘çš„ä¸€å¥—pythonåº“åŒ…ï¼Œå®ƒæä¾›ä¸€ä¸ªå¹³å°æ¡†æ¶ï¼Œä½¿å¾—transformeræŠ€æœ¯å®ç°çš„å„ç±»æ¨¡å‹èƒ½é€šè¿‡ä¸€è‡´åŒ–çš„æ¥å£æ–¹å¼å‘ˆç°å’Œè°ƒç”¨ã€‚åœ¨å…¶ä¸»é¡µä¸Šå®ƒæ˜¯è¿™ä¹ˆè‡ªæˆ‘ä»‹ç»çš„ï¼š\n",
    "\n",
    "ğŸ¤— Transformers æä¾› API æ¥è½»æ¾ä¸‹è½½å’Œè®­ç»ƒæœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥é™ä½æ‚¨çš„è®¡ç®—æˆæœ¬ã€ç¢³è¶³è¿¹ï¼Œå¹¶èŠ‚çœæ‚¨ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹çš„æ—¶é—´ã€‚è¿™äº›æ¨¡å‹å¯ç”¨äºä¸åŒçš„æ¨¡å¼ï¼Œä¾‹å¦‚ï¼š\n",
    "\n",
    "ğŸ“ æ–‡æœ¬ï¼šè¶…è¿‡ 100 ç§è¯­è¨€çš„æ–‡æœ¬åˆ†ç±»ã€ä¿¡æ¯æå–ã€é—®ç­”ã€æ‘˜è¦ã€ç¿»è¯‘å’Œæ–‡æœ¬ç”Ÿæˆã€‚<br>\n",
    "ğŸ–¼ï¸ å›¾åƒï¼šå›¾åƒåˆ†ç±»ã€å¯¹è±¡æ£€æµ‹å’Œåˆ†å‰²ã€‚<br>\n",
    "ğŸ—£ï¸ éŸ³é¢‘ï¼šè¯­éŸ³è¯†åˆ«å’ŒéŸ³é¢‘åˆ†ç±»ã€‚<br>\n",
    "ğŸ™ å¤šæ¨¡æ€ï¼šè¡¨æ ¼é—®ç­”ã€å…‰å­¦å­—ç¬¦è¯†åˆ«ã€ä»æ‰«ææ–‡æ¡£ä¸­æå–ä¿¡æ¯ã€è§†é¢‘åˆ†ç±»å’Œè§†è§‰é—®ç­”ã€‚<br>\n",
    "è¯¥åº“æ”¯æŒä¸‰ä¸ªæœ€æµè¡Œçš„æ·±åº¦å­¦ä¹ åº“ä¹‹é—´çš„æ— ç¼é›†æˆï¼šPyTorchã€TensorFlowå’ŒJAXã€‚åœ¨ä¸€ä¸ªæ¡†æ¶ä¸­ç”¨ä¸‰è¡Œä»£ç è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååŠ è½½å®ƒä»¥ä¸å¦ä¸€ä¸ªæ¡†æ¶è¿›è¡Œæ¨ç†ã€‚\n",
    "\n",
    "æ¯ä¸ªğŸ¤— Transformers æ¶æ„éƒ½åœ¨ç‹¬ç«‹çš„ Python æ¨¡å—ä¸­å®šä¹‰ï¼Œå› æ­¤å¯ä»¥è½»æ¾å®šåˆ¶å®ƒä»¬ä»¥è¿›è¡Œç ”ç©¶å’Œå®éªŒã€‚\n",
    "\n",
    "***æœ¬è´¨ä¸Šå®ƒæ˜¯ä¸€ç§ç ”å‘å·¥å…·äº§å“ã€‚*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f07801f",
   "metadata": {},
   "source": [
    "### BERT-wwm-ext \n",
    "\n",
    "Googleåœ¨å‘å¸ƒå…¬å¼€è®ºæ–‡ã€Attention Is All You Needã€çš„åŒæ—¶ï¼Œæ¨å‡ºäº†å¼€æºçš„transformeræ¶æ„**BERT**ï¼ˆBidirectional Encoder Representation from Transformersï¼‰ã€‚å¯ä»¥è¯´BERTå¼€è¾Ÿäº†NLPçš„æ–°æ—¶ä»£ï¼Œäº§å­¦ç ”ç•Œå‚è€ƒBERTæ¨å‡ºäº†å¾ˆå¤šç±»ä¼¼çš„å˜ç§æ¨¡å‹ï¼Œå¦‚ï¼š**RoBERTaã€BART**ç­‰ã€‚ \n",
    "\n",
    "åœ¨ä¸­æ–‡çš„ç±»**BERT**ç ”ç©¶ä¸­ï¼Œé™¤äº†GoogleåŸç”Ÿçš„**bert-base-chinese**ä¹‹å¤–ï¼Œ**Bert-wwmã€MacBertã€ChineseBert**ç­‰æ¨¡å‹åˆ›æ–°äº†ä¸åŒçš„æœºåˆ¶è¿›è¡Œä¼˜åŒ–ï¼Œå…·æœ‰æ¯”è¾ƒå¤§çš„çŸ¥ååº¦å’Œå½±å“åŠ›ã€‚ \n",
    "\n",
    "æˆ‘ä»¬ä»Šå¤©å°†åŸºäºå“ˆå·¥å¤§ä¸ç§‘å¤§è®¯é£è”åˆå®éªŒå®¤ï¼ˆHFLï¼‰ç ”å‘çš„**Bert-wwm-ext**è¿›è¡Œå¾®è°ƒï¼Œå®ç°æˆ‘ä»¬çš„åœºæ™¯ä»»åŠ¡ç›®æ ‡â€”â€”è¾¨åˆ«æ–°é—»æ ‡é¢˜Aå’ŒBçš„å…³ç³»åˆ†ç±»ã€‚\n",
    "ä¸åŸå§‹çš„**bert-base-chinese**ä¸­æ–‡é‡‡ç”¨å•å­—æ©ç æ–¹å¼ä¸åŒï¼Œ**Bert-wwm-ext**æä¾›äº†æ‰€è°“å…¨è¯æ©ç ï¼ˆWhole Word Maskingï¼‰çš„é¢„è®­ç»ƒæ–¹å¼ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n",
    "![è¿™æ˜¯å›¾ç‰‡](imgs/masks.png \"Masks\")\n",
    "\n",
    "å…¨è¯æ©ç æœ‰ä¸¤ä¸ªä¼˜ç‚¹ï¼š<br>\n",
    "1ã€éƒ¨åˆ†è§£å†³äº†MLMç‹¬ç«‹æ€§å‡è®¾ï¼Œä½¿å¾—é¢„æµ‹tokenä¹‹é—´æ‹¥æœ‰äº†ä¸€å®šçš„å…³è”æ€§<br>\n",
    "2ã€æé«˜äº†MLMä»»åŠ¡éš¾åº¦ï¼Œä½¿å¾—æ¨¡å‹éœ€è¦æ›´å¤šä¾èµ–è¿œè·ç¦»çš„ä¸Šä¸‹æ–‡æ¥åˆ¤æ–­æ©ç éƒ¨åˆ†çš„å†…å®¹<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c924598",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥å°†çœ‹åˆ°ï¼Œä½¿ç”¨ğŸ¤— Transformers åº“å°†ä½¿æˆ‘ä»¬çš„ä»£ç éå¸¸ç®€åŒ–ï¼šè™½ç„¶**Bert-wwm-ext** çš„ç½‘ç»œç»“æ„çš„å¤æ‚ç¨‹åº¦è¦è¿œè¿œè¶…è¿‡å‰é¢çš„å­ªç”ŸLSTMç½‘ç»œï¼Œä½†å®ç°çš„ä»£ç è¡Œæ•°å´ä¼šå¤§å¤§é™ä½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7439139",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:92: FutureWarning: Deprecated argument(s) used in 'snapshot_download': ignore_regex. Will not be supported from version '0.12'.\n",
      "\n",
      "Please use `allow_patterns` and `ignore_patterns` instead.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\HP/.cache\\\\huggingface\\\\hub\\\\models--hfl--chinese-bert-wwm-ext\\\\snapshots\\\\2a995a880017c60e4683869e817130d8af548486'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "#ä»å®˜æ–¹çš„huggingface_hubä¸‹è½½æ¨¡å‹é…ç½®ã€å‚æ•°ã€æ¨¡å‹è¯åº“ç­‰ä¿¡æ¯\n",
    "\n",
    "snapshot_download(repo_id=\"hfl/chinese-bert-wwm-ext\", ignore_regex=[\"*.h5\", \"*.ot\", \"*.msgpack\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f1ed25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\HP/.cache\\huggingface\\hub\\models--hfl--chinese-Bert-wwm-ext\\snapshots\\2a995a880017c60e4683869e817130d8af548486 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at C:\\Users\\HP/.cache\\huggingface\\hub\\models--hfl--chinese-Bert-wwm-ext\\snapshots\\2a995a880017c60e4683869e817130d8af548486 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig,AutoTokenizer,AutoModel,AutoModelForSequenceClassification\n",
    "model_name = 'C:\\\\Users\\\\HP/.cache\\\\huggingface\\\\hub\\\\models--hfl--chinese-Bert-wwm-ext\\\\snapshots\\\\2a995a880017c60e4683869e817130d8af548486' \n",
    "config = AutoConfig.from_pretrained(model_name) \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3) #æ–°é—»æ ‡é¢˜Aå’ŒBçš„å…³ç³»æ ‡ç­¾æœ‰3ç§ç±»å‹ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d4cae",
   "metadata": {},
   "source": [
    "å…ˆé€šè¿‡ä¸€ä¸ªå°ä¾‹å­çœ‹çœ‹ğŸ¤— Transformersçš„ **tokenizer** æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7230dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ab = [\n",
    "    (\"æˆ‘ä»Šå¤©è¾“æ¶²äº†\", \"è¾“ä»€ä¹ˆæ¶²?\"),\n",
    "    (\"è®©æˆ‘å¥½å¥½çˆ±ä½ è¡Œä¸ï¼Ÿ\", \"è®©æˆ‘é™ªä½ ä¸€èµ·è¿‡æ—¥å­å¥½ä¸ï¼Ÿ\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30e565cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2769,  791, 1921, 6783, 3890,  749,  102, 6783,  784,  720, 3890,\n",
       "          136,  102,    0,    0,    0,    0,    0,    0],\n",
       "        [ 101, 6375, 2769, 1962, 1962, 4263,  872, 6121,  679,  102, 6375, 2769,\n",
       "         7373,  872,  671, 6629, 6814, 3189, 2094,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(list_ab, padding=True, truncation=True, max_length=20, return_tensors='pt')\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33890666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] è®© æˆ‘ å¥½ å¥½ çˆ± ä½  è¡Œ ä¸ [SEP] è®© æˆ‘ é™ª ä½  ä¸€ èµ· è¿‡ æ—¥ å­ [SEP]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc293482",
   "metadata": {},
   "source": [
    "åœ¨æ­¤label ä¸éœ€è¦äººå·¥åš **One-hot** ç¼–ç ï¼Œåç»­æ¨¡å‹è®­ç»ƒä¸­ä¼šè‡ªåŠ¨åŒ–è¿›è¡Œç›¸å…³å¤„ç†ã€‚ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a67323",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_index = { 'unrelated' : 0 , 'agreed' : 1 , 'disagreed' : 2 } \n",
    "label_pipeline = lambda x : label_to_index [ x ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03250960",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 60 \n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "BATCH_SIZE = 64 \n",
    "LEARNING_RATE = 2e-5 \n",
    "EPOCHS=2 \n",
    "WEIGHT_DECAY=0.01 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb4822",
   "metadata": {},
   "source": [
    "**Huggningface transformers** ä¸**Huggingface** è‡ªå·±çš„æ•°æ®é›†å¤„ç†åŒ…**datasets** é›†æˆè¾ƒå¥½ã€‚ \n",
    "**Bert-wwm-ext** å°†ç›´æ¥å¤„ç†ä¸­æ–‡æ–‡æœ¬è¯­æ–™ï¼Œå› æ­¤ä¹‹å‰çš„ç»“å·´åˆ†è¯ç»“æœä¹Ÿä¸å†éœ€è¦ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨**pandas** çš„**read_csv** å‡½æ•° + **datasets** åŒ…é‡Œçš„**load_dataset** å‡½æ•°åŠ è½½å’Œå¤„ç†åŸå§‹çš„**.csv** ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e4221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV_PATH = \"./train.csv\" \n",
    "import pandas as pd \n",
    "train = pd.read_csv(TRAIN_CSV_PATH, index_col = 0) \n",
    "cols = ['title1_zh', 'title2_zh', 'label'] \n",
    "\n",
    "train = train.loc[:, cols].fillna('') \n",
    "train.rename(columns={'label':'label_class'},inplace=True) # é‡è¦ï¼å­—æ®µåç§°é‡Œéœ€è¦ç©ºå‡ºtransformersè§„å®šçš„ã€Œlabelã€ä¿ç•™å­— "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "594b45ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "dataset = Dataset.from_pandas(train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04df51b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title1_zh', 'title2_zh', 'label_class', 'id'],\n",
       "    num_rows: 320552\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107bb3e4",
   "metadata": {},
   "source": [
    "å¯¹æ•°æ®åšäº›åŸºæœ¬çš„å¤„ç†ï¼Œç”Ÿæˆæˆå¯¹çš„æ–°é—»æ ‡é¢˜Aå’ŒBçš„æ–‡æœ¬ï¼Œä»¥åŠæ•°å­—åŒ–çš„å…³ç³»åˆ†ç±»æ ‡ç­¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62a00798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bea0695d32e4fd6a1377be1e5cd2b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/321 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples): \n",
    "    #print(len(examples[\"label_class\"])) \n",
    "    #print(examples) \n",
    "    labels = [label_pipeline(label) for label in examples[\"label_class\"]] \n",
    "    #print(labels) \n",
    "       \n",
    "    texts = [(examples[\"title1_zh\"][i],examples[\"title2_zh\"][i]) for i in range(len(examples[\"label_class\"]))] \n",
    "    #print(texts) \n",
    "\n",
    "    tokenized = tokenizer(texts, padding='max_length', truncation=True, max_length=MAX_LEN) \n",
    "    tokenized['label'] = labels #transformersçš„æ¨¡å‹é€šè¿‡ã€Œlabelã€å­—æ®µä¼ é€’åˆ†ç±»æ ‡ç­¾ \n",
    "    return tokenized \n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d681b6",
   "metadata": {},
   "source": [
    "æ‰‹å†™ä¸€ä¸ªç®€å•çš„æ•°æ®é›†åˆ†å‰²åŠŸèƒ½ï¼Œå› ä¸ºæ˜¯cpuè®­ç»ƒï¼Œä¸ºèŠ‚çœè®­ç»ƒæ—¶é—´ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨è·Ÿæµ‹è¯•éªŒè¯é›†åŒæ ·å¤šçš„ã€å æ€»ä½“10%çš„æ ·æœ¬è¿›è¡Œæ¨¡å‹å¾®è°ƒè®­ç»ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b213f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(tokenized_datasets['label']) \n",
    "test_size = int(TEST_SPLIT * dataset_size) \n",
    "train_size = dataset_size - test_size \n",
    "train_dataset = tokenized_datasets.shuffle(seed=42).select(range(test_size)) #ä¸ºèŠ‚çœè®­ç»ƒæ—¶é—´ï¼Œä»…ä½¿ç”¨10%çš„æ ·æœ¬è®­ç»ƒ \n",
    "test_dataset = tokenized_datasets.shuffle(seed=42).select(range(test_size, 2* test_size)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96738609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title1_zh', 'title2_zh', 'label_class', 'id', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "    num_rows: 32055\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a40a5614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "# å°†æ¨¡å‹å’Œæ•°æ®è½¬ç§»åˆ°cuda, è‹¥æ— cuda,å¯æ›´æ¢ä¸ºcpu \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = model.to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe52f4a",
   "metadata": {},
   "source": [
    "Traineråœ¨è®­ç»ƒæœŸé—´ä¸ä¼šè‡ªåŠ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚éœ€è¦å‘Trainerä¼ é€’ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—å’ŒæŠ¥å‘ŠæŒ‡æ ‡ã€‚ğŸ¤— Datasets åº“æä¾›äº†ä¸€ä¸ªç®€å•çš„å‡½æ•° **load_metric** åŠ è½½ã€‚ \n",
    "å®šä¹‰ä¸€ä¸ªcompute_metricè®¡ç®—é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bbad1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from datasets import load_metric \n",
    "\n",
    "metric = load_metric(\"accuracy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "245861e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred): \n",
    "    logits, labels = eval_pred \n",
    "    predictions = np.argmax(logits, axis=-1) \n",
    "    return metric.compute(predictions=predictions, references=labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd7d3bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc06ab6f",
   "metadata": {},
   "source": [
    "æ­¤æ—¶ï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š\n",
    "\n",
    "åœ¨**TrainingArguments** ä¸­å®šä¹‰è®­ç»ƒè¶…å‚æ•°ã€‚\n",
    "å°†è®­ç»ƒå‚æ•°è¿åŒæ¨¡å‹ã€æ•°æ®é›†ã€æ ‡è®°å™¨å’Œæ•°æ®æ•´ç†å™¨ä¸€èµ·ä¼ é€’ç»™**Trainer** ã€‚\n",
    "è°ƒç”¨**train()** æ¥å¾®è°ƒæ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da6a1690",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: title2_zh, title1_zh, label_class, id. If title2_zh, title1_zh, label_class, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 32055\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1002\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1002' max='1002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1002/1002 7:32:16, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.331700</td>\n",
       "      <td>0.286503</td>\n",
       "      <td>0.875838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.214100</td>\n",
       "      <td>0.286773</td>\n",
       "      <td>0.882546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: title2_zh, title1_zh, label_class, id. If title2_zh, title1_zh, label_class, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 32055\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: title2_zh, title1_zh, label_class, id. If title2_zh, title1_zh, label_class, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 32055\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1002, training_loss=0.2726947990303744, metrics={'train_runtime': 27155.7832, 'train_samples_per_second': 2.361, 'train_steps_per_second': 0.037, 'total_flos': 1976742329360400.0, 'train_loss': 0.2726947990303744, 'epoch': 2.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments( \n",
    "    output_dir=\"./results\", \n",
    "    evaluation_strategy=\"epoch\", \n",
    "    learning_rate=LEARNING_RATE, \n",
    "    per_device_train_batch_size=BATCH_SIZE, \n",
    "    per_device_eval_batch_size=BATCH_SIZE, \n",
    "    num_train_epochs=EPOCHS, \n",
    "    weight_decay=WEIGHT_DECAY, \n",
    ") \n",
    "\n",
    "trainer = Trainer( \n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=test_dataset, \n",
    "    compute_metrics=compute_metrics, \n",
    ") \n",
    "\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fca2a0",
   "metadata": {},
   "source": [
    "ç»è¿‡2è½®è®­ç»ƒä¹‹åçš„å‡†ç¡®ç‡æ˜¯<font color=\"#dd0000\">88.3%</font>ï¼Œæ¯”ä¹‹å‰æœ€ä½³çš„å­ªç”ŸLSTMæ¨¡å‹<font color=\"#dd0000\">83.1%</font>åˆæå‡äº†è¶…è¿‡5ä¸ªç‚¹ï¼Œè€Œè¿™åªæ˜¯ä½¿ç”¨äº†10%çš„æ ·æœ¬è¿›è¡Œå¾®è°ƒè®­ç»ƒï¼Œè€ŒéLSTMæ¨¡å‹ä½¿ç”¨90%çš„æ ·æœ¬è®­ç»ƒï¼ \n",
    "\n",
    "---\n",
    "\n",
    "è‡³æ­¤ï¼Œæœ¬ç³»åˆ—åˆ†äº«å®Œæ¯•ã€‚ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
