{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad35e0e",
   "metadata": {},
   "source": [
    "## `从RNN和LSTM开始理解深度学习网络`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d432bb",
   "metadata": {},
   "source": [
    "看到这个标题，是不是已经觉得有点高级了？……在系列的下一期将会呈现针对我们的业务目标——「辨别新闻标题A和B的关系」的完整的LSTM模型神经网络训练和测试代码。在那之前，我们将试图把原理部分解释清楚。因此，这期内容没有代码。\n",
    "\n",
    "### 深度学习\n",
    "\n",
    "尽管很容易从网上搜到，这里还是简单介绍一下深度学习的概念： \n",
    "深度学习是机器学习的一种，深度学习的概念源于人工神经网络的研究，含多个隐藏层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。研究深度学习的动机在于建立模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本等。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e087f0",
   "metadata": {},
   "source": [
    "### RNN 网络\n",
    "\n",
    "针对业务目标，我们将使用循环神经网路（Recurrent Neural Network, 后简称RNN）来处理embedding之后的序列数据。 \n",
    "RNN 是一种有「记忆力」的神经网路，其最为人所知的形式如下：\n",
    "![这是图片](imgs/rnn.png \"RNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850cf58c",
   "metadata": {},
   "source": [
    "如同上图等号左侧所示，RNN 跟一般深度学习中常见的前馈神经网路（Feedforward Neural Network, 后简称FFNN）最不一样的地方在于它有一个循环（Loop）。 \n",
    "要了解这个循环在RNN 里头怎么运作，现在让我们想像有一个输入序列X（Input Sequence）其长相如下： \n",
    "X = [ x0 , x1 , x2 , ... xt ]  \n",
    "\n",
    "在第一个时间点t0，RNN 只将该序列中的第一个元素 x0 读入中间的单元A。单元A 则会针对 x0 做些处理以后，更新自己的「状态」并输出第一个结果h0。 \n",
    " \n",
    "在下个时间点t1，RNN 如法炮制，读入序列X 中的下一个元素x1，并利用刚刚处理完 x0 得到的单元状态，处理 x1 并更新自己的状态（也被称为记忆），接着输出另个结果h1。 \n",
    "\n",
    "剩下的 xt 都会被以同样的方式处理。循环代表单元A 利用「上」一个时间点储存的状态，来处理当下时间点的输入。 \n",
    " \n",
    "现在你可以想像为何RNN 非常适合拿来处理像是自然语言这种序列数据了。 \n",
    "就像你是由左到右逐字在大脑里处理我现在写的文字，同时不断地更新你脑中的记忆状态。 \n",
    "每当下个词汇映入眼中，你脑中的处理都会跟以下两者相关： \n",
    "•\t前面所有已读的词汇 \n",
    "•\t目前脑中的记忆状态 \n",
    "当然，实际人脑的阅读机制更为复杂，但RNN 抓到这个处理精髓，利用内在循环以及单元内的「记忆状态」来处理序列数据。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec5e21",
   "metadata": {},
   "source": [
    "### 记忆力更好的LSTM \n",
    "\n",
    "在最原始的RNN 的单元A 里，对于当下时刻t的记忆 state_t 与输入 input_t 的处理逻辑很简单。而这导致其记忆状态 state_t 没办法很好地「记住」前面处理过的序列元素，造成RNN 在处理后来的元素时，就已经把前面重要的资讯给忘记了。 \n",
    "这就好像一个人在讲了好长一段话以后，忘了前面到底讲过些什么的情境。 \n",
    "\n",
    "长短期记忆（Long Short-Term Memory, 后简称LSTM）就是被设计来解决RNN 的这个问题。 \n",
    "如下图所示，你可以把LSTM 想成是RNN 中用来实现单元A 内部处理逻辑的一个特定方法：\n",
    "![这是图片](imgs/lstm.png \"LSTM\")\n",
    "以抽象的层次来看，LSTM 就是实现RNN 中单元A 逻辑的一个方式（图片来源） \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7018559c",
   "metadata": {},
   "source": [
    "LSTM 单元里会有3 个闸门（Gates）来控制单元在不同时间点的记忆状态： \n",
    "\n",
    "•\tForget Gate：决定单元是否要遗忘目前的记忆状态<br> \n",
    "•\tInput Gate：决定目前输入有没有重要到值得处理<br> \n",
    "•\tOutput Gate：决定更新后的记忆状态有多少要输出<br> \n",
    "\n",
    "透过这些闸门控管机制，LSTM 可以将很久以前的记忆状态储存下来，让LSTM 即使面对很长的序列数据也能有效处理，不遗忘以前的记忆。 \n",
    "因为效果卓越，LSTM 非常广泛地被使用。当有人跟你说他用RNN 做了什么NLP 项目时，有9 成概率他是使用LSTM 或是GRU（LSTM 的改良版，只使用2 个闸门）来实现的。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d9326",
   "metadata": {},
   "source": [
    "### 孪生神经网络（Siamese Network）架构\n",
    "\n",
    "不同于一般的神经网络只接受一个数据来源作为输入，我们想要的是一个能够读入成对新闻标题，并判断两者之间关系的神经网络架构。\n",
    "我们可以使用孪生神经网络（Siamese Network）架构来处理这一问题： \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665254e",
   "metadata": {},
   "source": [
    "![这是图片](imgs/siamese.jpg \"siamese\")\n",
    "\n",
    "这张图是引用李孟博文里原创的他认为最重要的一张图。<br> \n",
    "在孪生神经网络架构里：一部份的神经网络（红框部分）被重复用来处理多个不同的数据来源（在本篇中为2 篇不同的新闻标题的token序列）。<br> \n",
    "标题A 跟标题B 的数据结构很像，我们并不需要两个不同的embedding和LSTM 来分别对A与B做处理，而是只需要让A与B共享一个网络即可。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625bcbc6",
   "metadata": {},
   "source": [
    "---\n",
    "好了，有了这些知识储备，可以期待下一期的深度学习代码呈现了。 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
