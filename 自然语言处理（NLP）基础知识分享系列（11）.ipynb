{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad35e0e",
   "metadata": {},
   "source": [
    "## `Transformer改变一切`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d432bb",
   "metadata": {},
   "source": [
    "前面我们已经学习了针对我们的业务目标——「辨别新闻标题A和B的关系」，如何训练一个LSTM孪生神经网络模型，并获得较好的测试验证结果。 \n",
    "这期我们跟着NLP技术路线的进化，认识一下一个划时代的全新网络「Transformer」。是的，名不虚传，它真的就像「变形金刚」一样厉害。 \n",
    "同样，今天的内容没有代码。\n",
    "\n",
    "### Attention Is All You Need \n",
    "\n",
    "Transformer的故事开始于Google的一篇公开论文『Attention Is All You Need』，https://arxiv.org/abs/1706.03762 。 \n",
    "对于这篇巨作，网上有很多解读性的文章。本系列不会花太多篇幅重述，只做简单介绍。如果想完整了解Transformer 和 Attention的技术细节，可自行搜索阅读。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e087f0",
   "metadata": {},
   "source": [
    "### Transformer是记忆力更更好的网络\n",
    "\n",
    "我们可以回忆一下不管RNN 或是LSTM ，都是对于当下时刻t的记忆 state_t 与输入 input_t的信息进行合成处理。 \n",
    "而这个「记忆」是沿着时间线逐步积累到当下时刻的。 \n",
    "\n",
    "Transformer 又如何呢？请看下图： \n",
    "\n",
    "![这是图片](imgs/photo1.png \"Transformer attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850cf58c",
   "metadata": {},
   "source": [
    "回想一下，RNN 和LSTM 的工作方式就像是在大脑里处理由左到右逐字读到的文字。 \n",
    "Transformer呢？ 它像是一个「一目十行」的超人。它无视方向和距离，使得当下的词有机会直接和句子中的每个词建立关联。 \n",
    "\n",
    "而这个「机会」的大小，就是上面提到的「注意力」（Attention）。 \n",
    "就像上面的图中的连线，有粗有细。 \n",
    "\n",
    "Transformer的这个改变，除了彻底解决长时记忆的遗忘问题，另一个好处便是： \n",
    "处理当下输入的词，无需等待前续处理完毕。这样，所有的输入就可以并行处理，这使得网络的多层堆叠以及模型训练的GPU/TPU加速不再成为障碍。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec5e21",
   "metadata": {},
   "source": [
    "### 「自监督学习」的预训练 \n",
    "\n",
    "当模型通过堆叠变得越来越庞大时，就需要喂给它足够的「大数据」助它学习。 \n",
    "对于有监督学习来说，数据标注的成本非常得高。所幸Transformer通过一种借用文档已有上下文信息的「自监督」的学习方式进行预训练，有效规避了数据标注的问题。 \n",
    "一些常见的预训练模型包括：\n",
    "\n",
    "• MLM（Mask Language Model）：掩码语言建模，在句子中随机用[MASK]替换一部分的词，预训练模型输入每个词（含[MASK]）的编码信息，以[MASK]位置的模型输出预测[MASK]原本的正确词 \n",
    "![这是图片](imgs/photo2.jpg \"MLM\")\n",
    "\n",
    "• CLM（Causal Language Model）：因果语言建模，是一种seq2seq自回归模型。该模型会预测句子下一个词（label与向右移动的输入token相同） "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d9326",
   "metadata": {},
   "source": [
    "### 「下游任务」的微调 \n",
    "\n",
    "预训练模型是在固定的任务下训练的模型，如：MLM。 但对于具体场景问题，数据的输入和输出是各不相同的。 \n",
    "但是预训练模型的参数，为这些「下游任务」的学习提供了巨人肩膀一般的起点。 \n",
    "仅适当改变网络头部的layer(s)结构，以及基于少量的场景数据进行迁移学习，对参数进行简单微调，就可以满足一个新的NLP任务要求。 \n",
    "\n",
    "![这是图片](imgs/photo3.jpg \"Down Streaming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426787c7",
   "metadata": {},
   "source": [
    "### 不断创造神迹的Transformer \n",
    "\n",
    "自从Transformer出来以后，便开始在NLP领域一统江湖。在2019年，微软、谷歌、Facebook团队基于Transformer的变种模型就在NLP测试基准GLUE中所有任务的总分上超过了人类的平均水平。近两年来，GLUE测试新的SOTA（state-of-the-art）模型记录仍在不断改写。像我们平常使用的一些能感知的NLP应用，如：苹果Siri、微软小冰、百度翻译等，背后也都是由Transformer模型在提供支撑。 \n",
    "![这是图片](imgs/photo4.png \"Records\")\n",
    "…… 更牛X的是，已有研究者开创性地将Transformer模型跨领域地引用到了计算机视觉任务中，并取得了不错地成果，在部分任务中达到了CNN SOTA的水准。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665254e",
   "metadata": {},
   "source": [
    "最后，上一张Transformer单元的近照。对照『Attention Is All You Need』慢慢研究吧。 \n",
    "\n",
    "![这是图片](imgs/photo5.png \"Transformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625bcbc6",
   "metadata": {},
   "source": [
    "---\n",
    "好了，就到这儿吧。 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
