{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ec78705",
   "metadata": {},
   "source": [
    "## `第一个机器学习模型` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e59550",
   "metadata": {},
   "source": [
    "上一回我们得到新闻标题文档的压缩到64维的LSI向量表示，我们用它来训练一个机器学习（Machine Learning）模型。 \n",
    "首先我们运行代码，重新在内存中加载它。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eca9a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pickle \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    " \n",
    "pkl_file_rb = open(r'./save_file', 'rb') \n",
    "train =pickle.load(pkl_file_rb) \n",
    " \n",
    "corpus = pd.concat([train . title1_tokenized, train . title2_tokenized]) \n",
    "corpus = [c for c in corpus] \n",
    " \n",
    "tfidf_model = TfidfVectorizer().fit(corpus) \n",
    "\n",
    "matrix1= tfidf_model.transform(train['title1_tokenized']) \n",
    "matrix2= tfidf_model.transform(train['title2_tokenized']) \n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd_model = TruncatedSVD(n_components=64, algorithm='randomized', n_iter=100, random_state=122) #参数n_components即降维的目标维数r \n",
    "svd_model.fit(tfidf_model.transform(corpus))\n",
    "\n",
    "matrix1_sub  = svd_model.transform(matrix1) \n",
    "matrix2_sub  = svd_model.transform(matrix2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84520a2e",
   "metadata": {},
   "source": [
    "我们把来自新闻标题A和B的LSI向量做一个差值，然后进行归一化处理。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a3c66c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320552, 64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  sklearn import preprocessing \n",
    "\n",
    "input_data = matrix1_sub - matrix2_sub \n",
    "input_data = preprocessing.scale(input_data) \n",
    "input_data.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9857710",
   "metadata": {},
   "source": [
    "### 将目标变量做数字化处理 \n",
    " \n",
    "我们已经将所有的新闻标题A和B以数字化表示，做成64维输入变量。 \n",
    "接下来需要将分类列 label 进行从文本到数字的转换。我们同样需要一个字典将分类的文字转换成索引。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da0b2208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_.shape:  (320552,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id\n",
       "10    1\n",
       "13    1\n",
       "12    1\n",
       "15    0\n",
       "17    2\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义每一个分类对应到的索引数字 \n",
    "label_to_index = { 'unrelated' : 0 , 'agreed' : 1 , 'disagreed' : 2 } \n",
    "\n",
    "# 将分类标签对应到刚定义的数字\n",
    "y_train_ = train . label . apply ( lambda x : label_to_index [ x ]) \n",
    " \n",
    "# y_train_ = np . asarray ( y_train_ ). astype ( 'float32' )  \n",
    "print('y_train_.shape: ',y_train_.shape) \n",
    "y_train_ [11: 16 ] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33519412",
   "metadata": {},
   "source": [
    "### 切割训练数据集& 验证数据集 \n",
    " \n",
    "我们需要将整个训练数据集（Training Set）的多少比例切出来当作验证数据集（Validation Set）。此例中我们用10 %。 \n",
    "但为何要再把本来的训练数据集切成2 个部分呢？ \n",
    "一般来说，我们在训练时只会让模型看到训练数据集，并用模型没看过的验证数据集来测试该模型在真实世界的表现。（毕竟我们没有测试数据集的答案） \n",
    "![这是图片](imgs/tvt.png \"train valid test\")\n",
    "\n",
    "要切训练数据集/ 验证数据集，**scikit-learn** 中的 **train_test_split** 函数是一个不错的选择： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc4f13c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (288496, 64)\n",
      "valid Shape: (32056, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection \n",
    " \n",
    "x_train, x_val, y_train, y_val = model_selection.train_test_split(input_data, y_train_, test_size = 0.1, random_state = 911) # 取10%做验证集 \n",
    " \n",
    "print(\"Train Shape: {}\".format(x_train.shape)) \n",
    "print(\"valid Shape: {}\".format(x_val.shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f2c240",
   "metadata": {},
   "source": [
    "### 训练机器学习模型 \n",
    "\n",
    "集成学习（ensemble learning），并不是一个单独的机器学习算法，而是通过构建并结合多个机器学习器（基学习器，Base learner）来完成学习任务。 \n",
    "对于训练集数据，我们通过训练若干个个体弱学习器（weak learner），通过一定的结合策略，就可以最终形成一个强学习器（strong learner），以达到博采众长的目的。 \n",
    "\n",
    "我们首先将建立一个随机森林模型，这是一个称为Bagging类型的集成类型算法。这个模型由几个“树”组成（我们将构造100棵树！）这将独立考虑每对新闻标题的LSI向量数据，可以并行运行。然后，随机森林模型做出一个民主决策，投票决定属于哪种分类关系。 \n",
    "![这是图片](imgs/bagging.png \"bagging\") \n",
    "我们使用 **scikit-learn** 中的 **RandomForestClassifier** 函数实现随机森林模型。 \n",
    "\n",
    "之后我们再建立一个梯度提升树模型，它属于所谓Boosting系列算法。多个学习器是依次构建的，个体学习器之间存在强依赖关系，需要串行生成。 \n",
    "![这是图片](imgs/boosting.png \"boosting\") \n",
    "我们使用 **scikit-learn** 中的 **GradientBoostingClassifier** 函数实现随机森林模型。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9715ca3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier \n",
    " \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    " \n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1) \n",
    "rfc.fit(x_train, y_train) \n",
    " \n",
    "predictions = rfc.predict(x_val) \n",
    "predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3da83",
   "metadata": {},
   "source": [
    "我们可以直接调用 **scikit-learn** 中的 **accuracy_score** 函数计算这个随机森林模型在验证集上的准确率。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce9c44af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurency of RandomForestClassifier: 71.5%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    " \n",
    "# acc_rfc = round(accuracy_score(predictions, np.eye(3)[y_val]) * 100, 2) \n",
    "acc_rfc = round(accuracy_score(predictions, y_val) * 100, 2) \n",
    "print('Accurency of RandomForestClassifier: {:.1f}%'.format(acc_rfc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "733b4389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurency of GradientBoostingClassifier: 73.0%\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Classifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    " \n",
    "gbk = GradientBoostingClassifier() \n",
    "gbk.fit(x_train, y_train) \n",
    "y_pred = gbk.predict(x_val) \n",
    "acc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2) \n",
    "print('Accurency of GradientBoostingClassifier: {:.1f}%'.format(acc_gbk)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fca2a0",
   "metadata": {},
   "source": [
    "效果比较好的是梯度提升树模型，准确率的结果是<font color=\"#dd0000\">73.0%</font>，与通过简单的LSI向量余弦距离方法仅有很微弱的提升，这可能是LSI数据供给的能力上限了。 \n",
    "\n",
    "之后，我们将从一个全新的视角来处理文档的数字化表示，力求保留下更完整的数据信息供模型所用。 \n",
    "\n",
    "---\n",
    "好了，就到这儿吧。 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
